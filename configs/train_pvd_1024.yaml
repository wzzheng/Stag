model:
  pretrained_checkpoint: "/home/user/Desktop/wln/diffusion/view/ViewCrafter-main/checkpoints/model_sparse.ckpt"
  base_learning_rate: 1.0e-05
  scale_lr: False
  target: lvdm.models.ddpm3d.VIPLatentDiffusion
  params:  
    rescale_betas_zero_snr: True
    parameterization: "v"
    linear_start: 0.00085
    linear_end: 0.012
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    first_stage_key: video
    cond_stage_key: caption
    cond_stage_trainable: False
    image_proj_model_trainable: True
    conditioning_key: hybrid
    image_size: [72, 128]
    channels: 4
    scale_by_std: False
    scale_factor: 0.18215
    use_ema: False
    uncond_prob: 0.05
    uncond_type: 'empty_seq'
    rand_cond_frame: true
    use_dynamic_rescale: true
    base_scale: 0.3
    fps_condition_type: 'fps'
    perframe_ae: True
    loop_video: Flase

    unet_config:
      target: lvdm.modules.networks.openaimodel3d.UNetModel
      params:
        in_channels: 8
        out_channels: 4
        model_channels: 320
        attention_resolutions:
        - 4
        - 2
        - 1
        num_res_blocks: 2
        channel_mult:
        - 1
        - 2
        - 4
        - 4
        dropout: 0.1
        num_head_channels: 64
        transformer_depth: 1
        context_dim: 1024
        use_linear: true
        use_checkpoint: True
        temporal_conv: True
        temporal_attention: True
        temporal_selfatt_only: true
        use_relative_position: false
        use_causal_attention: False
        temporal_length: 16
        addition_attention: true
        image_cross_attention: true
        default_fs: 10
        fs_condition: true

    first_stage_config:
      target: lvdm.models.autoencoder.AutoencoderKL
      params:
        embed_dim: 4
        monitor: val/rec_loss
        ddconfig:
          double_z: True
          z_channels: 4
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity

    cond_stage_config:
      target: lvdm.modules.encoders.condition.FrozenOpenCLIPEmbedder
      params:
        freeze: true
        layer: "penultimate"

    img_cond_stage_config:
      target: lvdm.modules.encoders.condition.FrozenOpenCLIPImageEmbedderV2
      params:
        freeze: true
    
    image_proj_stage_config:
      target: lvdm.modules.encoders.resampler.Resampler
      params:
        dim: 1024
        depth: 4
        dim_head: 64
        heads: 12
        num_queries: 16
        embedding_dim: 1280
        output_dim: 1024
        ff_mult: 4
        video_length: 16
        
        
data:
  target: utils_data.DataModuleFromConfig
  params:
    batch_size: 1
    num_workers: 32
    wrap: false
    train:
      target: lvdm.data.webvid.WebVid
      params:
        data_dir: <WebVid10M DATA>
        meta_path: <.csv FILE>
        video_length: 16
        frame_stride: 6
        load_raw_resolution: true
        resolution: [576, 1024]
        spatial_transform: resize_center_crop
        random_fs: true  ## if true, we uniformly sample fs with max_fs=frame_stride (above)
        
        
lightning:
  precision: 32
  strategy: deepspeed_stage_2
  trainer:
    benchmark: True
    accumulate_grad_batches: 2
    max_steps: 100000
    # logger
    log_every_n_steps: 5
    # val
    val_check_interval: 0.5
    gradient_clip_algorithm: 'norm'
    gradient_clip_val: 0.5
  callbacks:
    model_checkpoint:
      target: pytorch_lightning.callbacks.ModelCheckpoint
      params:
        every_n_train_steps: 500 #1000
        filename: "{epoch}-{step}"
        save_weights_only: True
    metrics_over_trainsteps_checkpoint:
      target: pytorch_lightning.callbacks.ModelCheckpoint
      params:
        filename: '{epoch}-{step}'
        save_weights_only: True
        every_n_train_steps: 300 #20000 # 3s/step*2w=
    batch_logger:
      target: callbacks.ImageLogger
      params:
        batch_frequency: 50000000
        to_local: False
        max_images: 8
        log_images_kwargs:
          ddim_steps: 50
          unconditional_guidance_scale: 7.5
          timestep_spacing: uniform_trailing
          guidance_rescale: 0.7
          
          
          
         
          
          
dataset_type: NuScenesTDataset
dataset_root: data/nuscenes/
dataset_process_root: ./data/mmdet_test/  # with visibility
dataset_cache_file_tag: null
dataset_cache_dirname: null
dataset_cache_file:
  - null  # for train_pipeline
  - null  # for test_pipeline

template: A driving scene image at {location}. {description}.
collect_meta_keys:  # send to DataContainer
  - camera_intrinsics
  - lidar2ego
  - lidar2camera
  - camera2lidar
  - lidar2image
  - img_aug_matrix
  # - lidar_aug_matrix  # this is useful when we change lidar and box
collect_meta_lis_keys:  # hold by one DataContainer
  - timeofday
  - location
  - description
  - filename
  - token

# point_cloud_range: [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
# voxel_size: [0.1, 0.1, 0.2]
# image_size: [256, 704]  # size of devfusion
image_size: [576, 1024]
map_bound:
  x: [-50.0, 50.0, 0.5]
  y: [-50.0, 50.0, 0.5]

view_order:
  - "CAM_FRONT_LEFT"
  - "CAM_FRONT"
  - "CAM_FRONT_RIGHT"
  - "CAM_BACK_RIGHT"
  - "CAM_BACK"
  - "CAM_BACK_LEFT"

neighboring_view_pair:
  0: [5, 1]
  1: [0, 2]
  2: [1, 3]
  3: [2, 4]
  4: [3, 5]
  5: [4, 0]

back_resize: [896, 1600]  # (h, w)
back_pad: [0, 4, 0, 0]  # left, top, right and bottom

augment2d:
  resize: [[0.25, 0.25]]
  rotate: null

aux_data:  # order is fix (v, c_offset, c_ohw, h), here only existence
  - visibility  # 1
  - center_offset  # 2
  - center_ohw  # 4 = 2 + 2
  - height  # height of 3d bbox
# augment2d:
#   resize: [[0.38, 0.55], [0.48, 0.48]]  # [train, test]
#   rotate: [-5.4, 5.4]
#   gridmask:
#     prob: 0.0
#     fixed_prob: true

augment3d:
  scale: [1.0, 1.0]  # adjust the scale
  rotate: [0.0, 0.0]  # rotation the lidar
  translate: 0  # shift
  flip_ratio: 0.0
  flip_direction: null

# class name conversion is done through pre-process
# re-order according to this list is done in NuScenesDataset.get_ann_info
object_classes:
  - car
  - truck
  - construction_vehicle
  - bus
  - trailer
  - barrier
  - motorcycle
  - bicycle
  - pedestrian
  - traffic_cone

map_classes:
  - drivable_area
  # - drivable_area*
  - ped_crossing
  - walkway
  - stop_line
  - carpark_area
  - road_divider
  - lane_divider
  # - divider
  - road_block

input_modality:
  use_lidar: false
  use_camera: true
  use_radar: false
  use_map: false
  use_external: false

train_pipeline:
  -
    type: LoadMultiViewImageFromFiles
    to_float32: true
  -
    type: LoadAnnotations3D
    with_bbox_3d: true
    with_label_3d: true
    with_attr_label: False
  -
    type: ImageAug3D  # random crop and rotate image and generate extrinsics
    final_dim: ${...image_size}
    resize_lim: ${...augment2d.resize[0]}  # range for resize ratio
    bot_pct_lim: [0.0, 0.0]  # this is the ratio in [0, 1] to keep bottom, default only crop top
    rot_lim: ${...augment2d.rotate}
    rand_flip: false
    is_train: false  # is false, range takes mean, disable flip and rotate
  -
    type: GlobalRotScaleTrans
    resize_lim: ${...augment3d.scale}
    rot_lim: ${...augment3d.rotate}
    trans_lim: ${...augment3d.translate}
    is_train: true
  -
    type: ObjectNameFilter  # this removes -1, do not really filter by names
    classes: ${...object_classes}
  -
    type: LoadBEVSegmentation
    dataset_root: ${...dataset_root}
    xbound: ${...map_bound.x}
    ybound: ${...map_bound.y}
    classes: ${...map_classes}
    #object_classes: null#${...object_classes}
    #aux_data: ${...aux_data}
    #cache_file: ${...dataset_cache_file.0}
  - 
    type: RandomFlip3DwithViews
    flip_ratio: ${...augment3d.flip_ratio}
    direction: ${...augment3d.flip_direction}
  # -
  #   type: RandomFlip3D  # random flip the whole lidar space
  # -
  #   type: PointsRangeFilter
  #   point_cloud_range: ${point_cloud_range}
  # -
  #   type: ObjectRangeFilter
  #   point_cloud_range: ${point_cloud_range}
  #- 
  #  type: ReorderMultiViewImagesM
  #  order: ${...view_order}
  #  safe: False
  -
    type: ImageNormalize
    mean: [0.5, 0.5, 0.5]
    std: [0.5, 0.5, 0.5]
  -
    type: DefaultFormatBundle3D
    classes: ${...object_classes}
  -
    type: Collect3D
    keys:  # keep as origin
      - img
      # - points
      - gt_bboxes_3d
      - gt_labels_3d
      #- instance_token_3d
      - gt_masks_bev
      #- gt_aux_bev
    meta_keys: ${...collect_meta_keys}
    meta_lis_keys: ${...collect_meta_lis_keys}

test_pipeline:
  -
    type: LoadMultiViewImageFromFiles
    to_float32: true
  -
    type: LoadAnnotations3D
    with_bbox_3d: true
    with_label_3d: true
    with_attr_label: False
  -
    type: ImageAug3D  # keep this to perform image resize
    final_dim: ${...image_size}
    resize_lim: ${...augment2d.resize[0]}
    bot_pct_lim: [0.0, 0.0]
    rot_lim: [0.0, 0.0]
    rand_flip: false
    is_train: false
  -
    type: GlobalRotScaleTrans  # add `lidar_aug_matrix`
    resize_lim: ${...augment3d.scale}
    rot_lim: ${...augment3d.rotate}
    trans_lim: ${...augment3d.translate}
    is_train: true
  -
    type: ObjectNameFilter
    classes: ${...object_classes}
  -
    type: LoadBEVSegmentation
    dataset_root: ${...dataset_root}
    xbound: ${...map_bound.x}
    ybound: ${...map_bound.y}
    classes: ${...map_classes}
    # object_classes: null
    #aux_data: ${...aux_data}
    #cache_file: ${...dataset_cache_file.1}
  # -
  #   type: PointsRangeFilter
  #   point_cloud_range: ${point_cloud_range}
  #- 
  #  type: ReorderMultiViewImagesM
  #  order: ${...view_order}
  #  safe: False
  -
    type: ImageNormalize
    mean: [0.5, 0.5, 0.5]
    std: [0.5, 0.5, 0.5]
  -
    type: DefaultFormatBundle3D
    classes: ${...object_classes}
  -
    type: Collect3D
    keys:
      - img
      # - points
      - gt_bboxes_3d
      - gt_labels_3d
      #- instance_token_3d
      - gt_masks_bev
      #- gt_aux_bev
    meta_keys: 
      - camera_intrinsics
      - lidar2ego
      - lidar2camera
      - camera2lidar
      - lidar2image
      - img_aug_matrix
      # - lidar_aug_matrix  # this is useful when we change lidar and box    
    meta_lis_keys: 
      - timeofday
      - location
      - description
      - filename
      - token

      
          
nus:
  template: A driving scene image at {location}. {description}.

  # samples_per_gpu: 4  # we do not set these here.
  # workers_per_gpu: 4
  train:  # here we drop the wrapper of CBGSDataset
    type: ${...dataset_type}
    dataset_root: ${...dataset_root}
    ann_file: ${...dataset_process_root}nuscenes_infos_train_t6.pkl
    pipeline: ${...train_pipeline}
    object_classes: ${...object_classes}
    map_classes: ${...map_classes}
    modality: ${...input_modality}
    test_mode: false
    # use_valid_flag: true  # this will filter some objects, not sure why
    #force_all_boxes: true  # !! even without `use_valid_flag`, objects with no lidar pts will be filtered
    box_type_3d: LiDAR  # this is ok, all boxes are under the lidar coord
    filter_empty_gt: false  # important, prevent from filter
  val:
    type: ${...dataset_type}
    dataset_root: ${...dataset_root}
    ann_file: ${...dataset_process_root}nuscenes_infos_val_t6.pkl
    pipeline: ${...test_pipeline}
    object_classes: ${...object_classes}
    map_classes: ${...map_classes}
    modality: ${...input_modality}
    test_mode: false
    #force_all_boxes: true  # !! even without `use_valid_flag`, objects with no lidar pts will be filtered
    box_type_3d: LiDAR
    filter_empty_gt: false  # important, prevent from filter
  test:
    type: ${...dataset_type}
    dataset_root: ${...dataset_root}
    ann_file: ${...dataset_process_root}nuscenes_infos_val.pkl
    pipeline: ${...test_pipeline}
    object_classes: ${...object_classes}
    map_classes: ${...map_classes}
    modality: ${...input_modality}
    test_mode: true
    #force_all_boxes: true  # !! even without `use_valid_flag`, objects with no lidar pts will be filtered
    box_type_3d: LiDAR
    filter_empty_gt: false  # important, prevent from filter    
    
modulemagic:
  name: SDv1.5mv-rawbox-t
  pretrained_model_name_or_path: #pretrained/stable-diffusion-v1-5/
  bbox_mode: 'all-xyz'
  bbox_view_shared: false
  crossview_attn_type: basic
  train_with_same_noise: false
  train_with_same_t: true
  
  load_pretrain_from: null
  allow_partial_load: false
  #pretrained_magicdrive: ../MagicDrive-pretrained/SDv1.5mv-rawbox_2023-09-07_18-39_224x400
  train_with_same_noise_t: false
  video_length: 3#7
  sc_attn_index:
    - [0, 0]  # keyframe
    - [0, 0]
    - [0, 1]
    - [0, 2]
    - [0, 3]
    - [0, 4]
    - [0, 5]  # keyframe

      
  runner_module: magicdrive.runner.multiview_t_runner.MultiviewTRunner

  pipe_module: magicdrive.pipeline.pipeline_bev_controlnet_t.StableDiffusionBEVControlNetTPipeline
  
  unet_module: magicdrive.networks.unet_2d_condition_multiview_t.UNet2DConditionModelMultiviewT
  use_fp32_for_unet_trainable: true
  unet_dir: unet
  unet:
    video_length: 3#7
    temp_attn_type: t_last
    temp_pos_emb: learnable
    zero_module_type2: zero_linear
    spatial_trainable: false
  
  model_module: magicdrive.networks.unet_addon_rawbox.BEVControlNetModel
  controlnet_dir: controlnet
  controlnet:
    # 7 param to embed: 3 for intrinsics + 4 for extrinsics
    # in_dim 3, num_freqs 4 -> 27 dim embedding
    camera_in_dim: 189
    camera_out_dim: 768
    map_size: [8, 200, 200]
    conditioning_embedding_out_channels: [16, 32, 96, 256]
  
    # for uncond camera param, learnable
    uncond_cam_in_dim: [3, 7]
    use_uncond_map: null  # negative1, random or learnable
    drop_cond_ratio: 0.25
    drop_cam_num: 6
    drop_cam_with_box: false
  
    cam_embedder_param:
      input_dims: 3
      num_freqs: 4  # nerf use 4 for view embedding
      include_input: True
      log_sampling: True
  
    bbox_embedder_cls: magicdrive.networks.bbox_embedder.ContinuousBBoxWithTextEmbedding
    bbox_embedder_param:
      n_classes: 10
      class_token_dim: 768
      trainable_class_token: false
      use_text_encoder_init: true
      embedder_num_freq: 4
      proj_dims: [768, 512, 512, 768]
      mode: ${...bbox_mode}
      minmax_normalize: false
